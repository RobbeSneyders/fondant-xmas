{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "import pandas as pd\n",
    "from src.fondant.pipeline import Pipeline\n",
    "\n",
    "from src.fondant.pipeline import Pipeline\n",
    "from src.fondant.pipeline.runner import DockerRunner\n",
    "from src.fondant.pipeline.compiler import DockerCompiler\n",
    "\n",
    "from src.fondant.component import PandasTransformComponent\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate some data\n",
    "data = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": [1, 2, 3],\n",
    "            \"y\": [4, 5, 6],\n",
    "        },\n",
    "        index=pd.Index([\"a\", \"b\", \"c\"], name=\"id\"),\n",
    "    )\n",
    "\n",
    "data.to_parquet(\"./foobar/sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-28 16:42:27,974 | src.fondant.pipeline.compiler | INFO] Compiling foo to docker-compose.yml\n",
      "[2023-12-28 16:42:27,975 | src.fondant.pipeline.compiler | INFO] Base path found on local system, setting up /Users/georgeslorre/ML6/internal/fondant-xmas/foobar as mount volume\n",
      "[2023-12-28 16:42:27,977 | src.fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2023-12-28 16:42:27,993 | src.fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2023-12-28 16:42:27,994 | src.fondant.pipeline.compiler | INFO] Compiling service for load_from_parquet\n",
      "[2023-12-28 16:42:27,995 | src.fondant.pipeline.compiler | INFO] Found Dockerfile for load_from_parquet, adding build step.\n",
      "[2023-12-28 16:42:28,001 | src.fondant.pipeline.compiler | INFO] Successfully compiled to docker-compose.yml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n",
      "#1 [load_from_parquet internal] load .dockerignore\n",
      "#1 transferring context: 2B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [load_from_parquet internal] load build definition from Dockerfile\n",
      "#2 transferring dockerfile: 650B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [load_from_parquet internal] load metadata for docker.io/library/python:3.8-slim\n",
      "#3 ...\n",
      "\n",
      "#4 [load_from_parquet auth] library/python:pull token for registry-1.docker.io\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [load_from_parquet internal] load metadata for docker.io/library/python:3.8-slim\n",
      "#3 DONE 1.6s\n",
      "\n",
      "#5 [load_from_parquet 1/7] FROM docker.io/library/python:3.8-slim@sha256:d1cba0f8754d097bd333b8f3d4c655f37c2ede9042d1e7db69561d9eae2eebfa\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [load_from_parquet internal] load build context\n",
      "#6 transferring context: 89B done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [load_from_parquet 6/7] WORKDIR /component/src\n",
      "#7 CACHED\n",
      "\n",
      "#8 [load_from_parquet 5/7] RUN pip3 install fondant[component,aws,azure,gcp]@git+https://github.com/ml6team/fondant@main\n",
      "#8 CACHED\n",
      "\n",
      "#9 [load_from_parquet 3/7] COPY requirements.txt /\n",
      "#9 CACHED\n",
      "\n",
      "#10 [load_from_parquet 2/7] RUN apt-get update &&     apt-get upgrade -y &&     apt-get install git -y\n",
      "#10 CACHED\n",
      "\n",
      "#11 [load_from_parquet 4/7] RUN pip3 install --no-cache-dir -r requirements.txt\n",
      "#11 CACHED\n",
      "\n",
      "#12 [load_from_parquet 7/7] COPY src/ .\n",
      "#12 CACHED\n",
      "\n",
      "#13 [load_from_parquet] exporting to image\n",
      "#13 exporting layers done\n",
      "#13 writing image sha256:a8fb05c1eb3e0ffa9e15bd0aff736e50a3766c25d1483efba4d9bb54c535ab80 done\n",
      "#13 naming to docker.io/library/foo-load_from_parquet done\n",
      "#13 DONE 0.0s\n",
      "Attaching to foo-load_from_parquet-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container foo-load_from_parquet-1  Recreate\n",
      " Container foo-load_from_parquet-1  Recreated\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:32,964 | fondant.cli | INFO] Component `LoadFromParquet` found in module main\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:32,975 | fondant.component.executor | INFO] Dask default local mode will be used for further executions.Our current supported options are limited to 'local' and 'default'.\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:32,977 | fondant.component.executor | INFO] No matching execution for component detected\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:32,977 | root | INFO] Executing component\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:32,977 | main | INFO] Loading dataset from the hub...\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:33,135 | main | INFO] Index column not specified, setting a globally unique index\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:33,181 | root | INFO] Creating write task for: /foobar/foo/foo-20231228164227/load_from_parquet\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:33,182 | root | INFO] Writing data...\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:33,299 | fondant.component.executor | INFO] Saving output manifest to /foobar/foo/foo-20231228164227/load_from_parquet/manifest.json\n",
      "foo-load_from_parquet-1  | [2023-12-28 15:42:33,299 | fondant.component.executor | INFO] Writing cache key to /foobar/foo/cache/9f97a9ff57e588abccb727a1cc56a5c6.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 680.46 us\n",
      "[########################################] | 100% Completed | 106.71 ms\n",
      "foo-load_from_parquet-1 exited with code 0\n",
      "Finished pipeline run.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(name=\"foo\", description=\"bar\", base_path=\"/Users/georgeslorre/ML6/internal/fondant-xmas/foobar\")\n",
    "\n",
    "\n",
    "dataset = pipeline.read(\n",
    "    name_or_path=\"components/load_from_parquet\",\n",
    "    arguments={\n",
    "        \"dataset_uri\": \"../../foobar/sample.parquet\",\n",
    "    },\n",
    "    produces={\"x\": pa.int32(), \"y\": pa.int32()},\n",
    ")\n",
    "\n",
    "DockerCompiler().compile(pipeline=pipeline)\n",
    "DockerRunner().run(input=\"./docker-compose.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addition(PandasTransformComponent):\n",
    "    def __init__(self, *_, **__):\n",
    "        pass\n",
    "\n",
    "    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        dataframe['z'] = dataframe['x'] + dataframe['y']\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n",
      "[2023-12-28 16:42:37,018 | fondant.component.executor | INFO] Dask default local mode will be used for further executions.Our current supported options are limited to 'local' and 'default'.\n",
      "[2023-12-28 16:42:37,020 | fondant.component.executor | INFO] Previous component `load_from_parquet` is not cached. Invalidating cache for current and subsequent components\n",
      "[2023-12-28 16:42:37,020 | fondant.component.executor | INFO] Caching disabled for the component\n",
      "[2023-12-28 16:42:37,020 | root | INFO] Executing component\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /foobar/foo/foo-20231228164227/load_from_parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/dask/backends.py:135\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/dask/dataframe/io/parquet/core.py:543\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     blocksize \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m read_metadata_result \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mread_metadata(\n\u001b[1;32m    544\u001b[0m     fs,\n\u001b[1;32m    545\u001b[0m     paths,\n\u001b[1;32m    546\u001b[0m     categories\u001b[39m=\u001b[39;49mcategories,\n\u001b[1;32m    547\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    548\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    549\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    550\u001b[0m     gather_statistics\u001b[39m=\u001b[39;49mcalculate_divisions,\n\u001b[1;32m    551\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    552\u001b[0m     split_row_groups\u001b[39m=\u001b[39;49msplit_row_groups,\n\u001b[1;32m    553\u001b[0m     blocksize\u001b[39m=\u001b[39;49mblocksize,\n\u001b[1;32m    554\u001b[0m     aggregate_files\u001b[39m=\u001b[39;49maggregate_files,\n\u001b[1;32m    555\u001b[0m     ignore_metadata_file\u001b[39m=\u001b[39;49mignore_metadata_file,\n\u001b[1;32m    556\u001b[0m     metadata_task_size\u001b[39m=\u001b[39;49mmetadata_task_size,\n\u001b[1;32m    557\u001b[0m     parquet_file_extension\u001b[39m=\u001b[39;49mparquet_file_extension,\n\u001b[1;32m    558\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset_options,\n\u001b[1;32m    559\u001b[0m     read\u001b[39m=\u001b[39;49mread_options,\n\u001b[1;32m    560\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_options,\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    563\u001b[0m \u001b[39m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:532\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m dataset_info \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_dataset_info(\n\u001b[1;32m    533\u001b[0m     paths,\n\u001b[1;32m    534\u001b[0m     fs,\n\u001b[1;32m    535\u001b[0m     categories,\n\u001b[1;32m    536\u001b[0m     index,\n\u001b[1;32m    537\u001b[0m     gather_statistics,\n\u001b[1;32m    538\u001b[0m     filters,\n\u001b[1;32m    539\u001b[0m     split_row_groups,\n\u001b[1;32m    540\u001b[0m     blocksize,\n\u001b[1;32m    541\u001b[0m     aggregate_files,\n\u001b[1;32m    542\u001b[0m     ignore_metadata_file,\n\u001b[1;32m    543\u001b[0m     metadata_task_size,\n\u001b[1;32m    544\u001b[0m     parquet_file_extension,\n\u001b[1;32m    545\u001b[0m     kwargs,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    548\u001b[0m \u001b[39m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/dask/dataframe/io/parquet/arrow.py:1047\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[39mif\u001b[39;00m ds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1047\u001b[0m     ds \u001b[39m=\u001b[39m pa_ds\u001b[39m.\u001b[39;49mdataset(\n\u001b[1;32m   1048\u001b[0m         paths,\n\u001b[1;32m   1049\u001b[0m         filesystem\u001b[39m=\u001b[39;49m_wrapped_fs(fs),\n\u001b[1;32m   1050\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_processed_dataset_kwargs,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/pyarrow/dataset.py:785\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    786\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(elem, Dataset) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/pyarrow/dataset.py:463\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 463\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    464\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/pyarrow/dataset.py:382\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mNotFound:\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(info\u001b[39m.\u001b[39mpath)\n\u001b[1;32m    383\u001b[0m \u001b[39melif\u001b[39;00m file_type \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mDirectory:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /foobar/foo/foo-20231228164227/load_from_parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mtb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m _ \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mexecute(component\u001b[39m=\u001b[39;49mAddition, produces\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mz\u001b[39;49m\u001b[39m\"\u001b[39;49m: pa\u001b[39m.\u001b[39;49mint32()}, consumes\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m: pa\u001b[39m.\u001b[39;49mint32(), \u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m: pa\u001b[39m.\u001b[39;49mint32()})\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/src/fondant/pipeline/pipeline.py:795\u001b[0m, in \u001b[0;36mDataset.execute\u001b[0;34m(self, component, requirements, consumes, produces, arguments, input_partition_rows, resources, cache, cluster_type, client_kwargs)\u001b[0m\n\u001b[1;32m    779\u001b[0m executor_mapping \u001b[39m=\u001b[39m  {\n\u001b[1;32m    780\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDaskLoadComponent\u001b[39m\u001b[39m\"\u001b[39m: DaskLoadExecutor,\n\u001b[1;32m    781\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDaskTransformComponent\u001b[39m\u001b[39m\"\u001b[39m: DaskTransformExecutor,\n\u001b[1;32m    782\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDaskWriteComponent\u001b[39m\u001b[39m\"\u001b[39m: DaskWriteExecutor,\n\u001b[1;32m    783\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPandasTransformComponent\u001b[39m\u001b[39m\"\u001b[39m: PandasTransformExecutor,\n\u001b[1;32m    784\u001b[0m }\n\u001b[1;32m    786\u001b[0m executor \u001b[39m=\u001b[39m executor_mapping[component\u001b[39m.\u001b[39m\u001b[39m__bases__\u001b[39m[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\u001b[39m.\u001b[39mfrom_spec(\n\u001b[1;32m    787\u001b[0m     component_spec,\n\u001b[1;32m    788\u001b[0m     cache\u001b[39m=\u001b[39mcache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     produces\u001b[39m=\u001b[39mproduces,\n\u001b[1;32m    794\u001b[0m     )\n\u001b[0;32m--> 795\u001b[0m executor\u001b[39m.\u001b[39;49mexecute(component)\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/src/fondant/component/executor.py:390\u001b[0m, in \u001b[0;36mExecutor.execute\u001b[0;34m(self, component_cls)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCaching disabled for the component\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m     output_manifest \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_execution(component_cls, input_manifest)\n\u001b[1;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupload_manifest(output_manifest, save_path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_manifest_path)\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/src/fondant/component/executor.py:360\u001b[0m, in \u001b[0;36mExecutor._run_execution\u001b[0;34m(self, component_cls, input_manifest)\u001b[0m\n\u001b[1;32m    354\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mExecuting component\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    355\u001b[0m component \u001b[39m=\u001b[39m component_cls(\n\u001b[1;32m    356\u001b[0m     consumes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation_spec\u001b[39m.\u001b[39minner_consumes,\n\u001b[1;32m    357\u001b[0m     produces\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation_spec\u001b[39m.\u001b[39minner_produces,\n\u001b[1;32m    358\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_arguments,\n\u001b[1;32m    359\u001b[0m )\n\u001b[0;32m--> 360\u001b[0m output_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_component(\n\u001b[1;32m    361\u001b[0m     component,\n\u001b[1;32m    362\u001b[0m     manifest\u001b[39m=\u001b[39;49minput_manifest,\n\u001b[1;32m    363\u001b[0m )\n\u001b[1;32m    364\u001b[0m output_manifest \u001b[39m=\u001b[39m input_manifest\u001b[39m.\u001b[39mevolve(\n\u001b[1;32m    365\u001b[0m     operation_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation_spec,\n\u001b[1;32m    366\u001b[0m     run_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mrun_id,\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_data(dataframe\u001b[39m=\u001b[39moutput_df, manifest\u001b[39m=\u001b[39moutput_manifest)\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/src/fondant/component/executor.py:563\u001b[0m, in \u001b[0;36mPandasTransformExecutor._execute_component\u001b[0;34m(self, component, manifest)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[39mLoad the data based on the manifest using a DaskDataloader and call the component's\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[39mtransform method for each partition of the data.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39m    A `dd.DataFrame` instance with updated data based on the applied data transformations.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m data_loader \u001b[39m=\u001b[39m DaskDataLoader(\n\u001b[1;32m    559\u001b[0m     manifest\u001b[39m=\u001b[39mmanifest,\n\u001b[1;32m    560\u001b[0m     operation_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation_spec,\n\u001b[1;32m    561\u001b[0m     input_partition_rows\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_partition_rows,\n\u001b[1;32m    562\u001b[0m )\n\u001b[0;32m--> 563\u001b[0m dataframe \u001b[39m=\u001b[39m data_loader\u001b[39m.\u001b[39;49mload_dataframe()\n\u001b[1;32m    565\u001b[0m \u001b[39m# Create meta dataframe with expected format\u001b[39;00m\n\u001b[1;32m    566\u001b[0m meta_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: pd\u001b[39m.\u001b[39mSeries(dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m)}\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/src/fondant/component/data_io.py:112\u001b[0m, in \u001b[0;36mDaskDataLoader.load_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m DEFAULT_INDEX_NAME \u001b[39min\u001b[39;00m fields:\n\u001b[1;32m    110\u001b[0m     fields\u001b[39m.\u001b[39mremove(DEFAULT_INDEX_NAME)\n\u001b[0;32m--> 112\u001b[0m partial_df \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39;49mread_parquet(\n\u001b[1;32m    113\u001b[0m     location,\n\u001b[1;32m    114\u001b[0m     columns\u001b[39m=\u001b[39;49mfields,\n\u001b[1;32m    115\u001b[0m     index\u001b[39m=\u001b[39;49mDEFAULT_INDEX_NAME,\n\u001b[1;32m    116\u001b[0m     calculate_divisions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m dataframe \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39m# ensure that the index is set correctly and divisions are known.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     dataframe \u001b[39m=\u001b[39m partial_df\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/.venv/lib/python3.10/site-packages/dask/backends.py:137\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\n\u001b[1;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling the \u001b[39m\u001b[39m{\u001b[39;00mfuncname(func)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmethod registered to the \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend\u001b[39m}\u001b[39;00m\u001b[39m backend.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /foobar/foo/foo-20231228164227/load_from_parquet"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "_ = dataset.execute(component=Addition, produces={\"z\": pa.int32()}, consumes={\"x\": pa.int32(), \"y\": pa.int32()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_parquet(\"/foobar/foo/foo-20231228164227/load_from_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
