{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from src.fondant.pipeline import Pipeline\n",
    "\n",
    "from src.fondant.pipeline.runner import DockerRunner\n",
    "from src.fondant.pipeline.compiler import DockerCompiler\n",
    "\n",
    "from src.fondant.component import PandasTransformComponent, DaskLoadComponent, DaskTransformComponent\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate some data\n",
    "data = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": [1, 2, 3],\n",
    "            \"y\": [4, 5, 6],\n",
    "        },\n",
    "        index=pd.Index([\"a\", \"b\", \"c\"], name=\"id\"),\n",
    "    )\n",
    "\n",
    "data.to_parquet(\"./foobar/sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(name=\"foo\", description=\"bar\", base_path=\"/Users/georgeslorre/ML6/internal/fondant-xmas/foobar\")\n",
    "\n",
    "\n",
    "dataset = pipeline.read(\n",
    "    name_or_path=\"components/load_from_parquet\",\n",
    "    arguments={\n",
    "        \"dataset_uri\": \"../../foobar/sample.parquet\",\n",
    "    },\n",
    "    produces={\"x\": pa.int32(), \"y\": pa.int32()},\n",
    ")\n",
    "\n",
    "DockerCompiler().compile(pipeline=pipeline)\n",
    "DockerRunner().run(input=\"./docker-compose.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addition(PandasTransformComponent):\n",
    "    def __init__(self, *_, **__):\n",
    "        pass\n",
    "\n",
    "    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        dataframe['z'] = dataframe['x'] + dataframe['y']\n",
    "        return dataframe\n",
    "\n",
    "Addition().__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb\n",
    "_ = dataset.execute(component=Addition, produces={\"z\": pa.int32()}, consumes={\"x\": pa.int32(), \"y\": pa.int32()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(\"./foobar/foo/foo-20231228164227/load_from_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This component loads a seed dataset from the hub.\"\"\"\n",
    "import logging\n",
    "import typing as t\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from fondant.component import DaskLoadComponent\n",
    "from fondant.core.schema import Field\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "\n",
    "\n",
    "class LoadFromParquet(DaskLoadComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        produces: t.Dict[str, Field],\n",
    "        dataset_uri: str,\n",
    "        column_name_mapping: t.Optional[dict],\n",
    "        n_rows_to_load: t.Optional[int],\n",
    "        index_column: t.Optional[str],\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            produces: The schema the component should produce\n",
    "            dataset_uri: The remote path to the parquet file/folder containing the dataset\n",
    "            column_name_mapping: Mapping of the consumed dataset to fondant column names\n",
    "            n_rows_to_load: optional argument that defines the number of rows to load. Useful for\n",
    "              testing pipeline runs on a small scale.\n",
    "            index_column: Column to set index to in the load component, if not specified a default\n",
    "                globally unique index will be set.\n",
    "            kwargs: Unhandled keyword arguments passed in by Fondant.\n",
    "        \"\"\"\n",
    "        self.dataset_uri = dataset_uri\n",
    "        self.column_name_mapping = column_name_mapping\n",
    "        self.n_rows_to_load = n_rows_to_load\n",
    "        self.index_column = index_column\n",
    "        self.produces = produces\n",
    "\n",
    "    def get_columns_to_keep(self) -> t.List[str]:\n",
    "        # Only read required columns\n",
    "        columns = []\n",
    "\n",
    "        if self.column_name_mapping:\n",
    "            invert_column_name_mapping = {\n",
    "                v: k for k, v in self.column_name_mapping.items()\n",
    "            }\n",
    "        else:\n",
    "            invert_column_name_mapping = {}\n",
    "\n",
    "        for field_name, field in self.produces.items():\n",
    "            column_name = field_name\n",
    "            if invert_column_name_mapping and column_name in invert_column_name_mapping:\n",
    "                columns.append(invert_column_name_mapping[column_name])\n",
    "            else:\n",
    "                columns.append(column_name)\n",
    "\n",
    "        if self.index_column is not None:\n",
    "            columns.append(self.index_column)\n",
    "\n",
    "        return columns\n",
    "\n",
    "    def set_df_index(self, dask_df: dd.DataFrame) -> dd.DataFrame:\n",
    "        if self.index_column is None:\n",
    "            logger.info(\n",
    "                \"Index column not specified, setting a globally unique index\",\n",
    "            )\n",
    "\n",
    "            def _set_unique_index(dataframe: pd.DataFrame, partition_info=None):\n",
    "                \"\"\"Function that sets a unique index based on the partition and row number.\"\"\"\n",
    "                dataframe[\"id\"] = 1\n",
    "                dataframe[\"id\"] = (\n",
    "                    str(partition_info[\"number\"])\n",
    "                    + \"_\"\n",
    "                    + (dataframe.id.cumsum()).astype(str)\n",
    "                )\n",
    "                dataframe.index = dataframe.pop(\"id\")\n",
    "                return dataframe\n",
    "\n",
    "            def _get_meta_df() -> pd.DataFrame:\n",
    "                meta_dict = {\"id\": pd.Series(dtype=\"object\")}\n",
    "                for field_name, field in self.produces.items():\n",
    "                    meta_dict[field_name] = pd.Series(\n",
    "                        dtype=pd.ArrowDtype(field.type.value),\n",
    "                    )\n",
    "                return pd.DataFrame(meta_dict).set_index(\"id\")\n",
    "\n",
    "            meta = _get_meta_df()\n",
    "            dask_df = dask_df.map_partitions(_set_unique_index, meta=meta)\n",
    "        else:\n",
    "            logger.info(f\"Setting `{self.index_column}` as index\")\n",
    "            dask_df = dask_df.set_index(self.index_column, drop=True)\n",
    "\n",
    "        return dask_df\n",
    "\n",
    "    def return_subset_of_df(self, dask_df: dd.DataFrame) -> dd.DataFrame:\n",
    "        if self.n_rows_to_load is not None:\n",
    "            partitions_length = 0\n",
    "            npartitions = 1\n",
    "            for npartitions, partition in enumerate(dask_df.partitions, start=1):\n",
    "                if partitions_length >= self.n_rows_to_load:\n",
    "                    logger.info(\n",
    "                        f\"\"\"Required number of partitions to load\\n\n",
    "                    {self.n_rows_to_load} is {npartitions}\"\"\",\n",
    "                    )\n",
    "                    break\n",
    "                partitions_length += len(partition)\n",
    "            dask_df = dask_df.head(self.n_rows_to_load, npartitions=npartitions)\n",
    "            dask_df = dd.from_pandas(dask_df, npartitions=npartitions)\n",
    "        return dask_df\n",
    "\n",
    "    def load(self) -> dd.DataFrame:\n",
    "        # 1) Load data, read as Dask dataframe\n",
    "        logger.info(\"Loading dataset from the hub...\")\n",
    "\n",
    "        columns = self.get_columns_to_keep()\n",
    "\n",
    "        logger.debug(f\"Columns to keep: {columns}\")\n",
    "        dask_df = dd.read_parquet(self.dataset_uri, columns=columns)\n",
    "\n",
    "        # 2) Rename columns\n",
    "        if self.column_name_mapping:\n",
    "            logger.info(\"Renaming columns...\")\n",
    "            dask_df = dask_df.rename(columns=self.column_name_mapping)\n",
    "\n",
    "        # 4) Optional: only return specific amount of rows\n",
    "        dask_df = self.return_subset_of_df(dask_df)\n",
    "\n",
    "        # 5) Set the index\n",
    "        dask_df = self.set_df_index(dask_df)\n",
    "        return dask_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LoadFromParquet.__init__() missing 5 required keyword-only arguments: 'produces', 'dataset_uri', 'column_name_mapping', 'n_rows_to_load', and 'index_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         dask_df \u001b[39m=\u001b[39m dd\u001b[39m.\u001b[39mread_parquet(\u001b[39m\"\u001b[39m\u001b[39m./foobar/sample.parquet\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m dask_df\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m dataset1 \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     component\u001b[39m=\u001b[39;49mLoadFromParquet,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     produces\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m: pa\u001b[39m.\u001b[39;49mint32(), \u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m: pa\u001b[39m.\u001b[39;49mint32()},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAddition\u001b[39;00m(PandasTransformComponent):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgeslorre/ML6/internal/fondant-xmas/main.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m__):\n",
      "File \u001b[0;32m~/ML6/internal/fondant-xmas/src/fondant/pipeline/pipeline.py:734\u001b[0m, in \u001b[0;36mDataset.execute\u001b[0;34m(self, component, requirements, consumes, produces, arguments, input_partition_rows, resources, cache, cluster_type, client_kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfondant\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponent\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecutor\u001b[39;00m \u001b[39mimport\u001b[39;00m ExecutorFactory\n\u001b[1;32m    732\u001b[0m \u001b[39m# get component name based on the class name\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m component_name \u001b[39m=\u001b[39m component()\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[39m# create a barebones component spec\u001b[39;00m\n\u001b[1;32m    737\u001b[0m spec_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    738\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: component_name,\n\u001b[1;32m    739\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mThis is an example component\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m     },\n\u001b[1;32m    747\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: LoadFromParquet.__init__() missing 5 required keyword-only arguments: 'produces', 'dataset_uri', 'column_name_mapping', 'n_rows_to_load', and 'index_column'"
     ]
    }
   ],
   "source": [
    "pipeline2 = Pipeline(name=\"foo\", description=\"bar\", base_path=\"/Users/georgeslorre/ML6/internal/fondant-xmas/foobar2\")\n",
    "\n",
    "dataset = pipeline2.read(\n",
    "    name_or_path=\"components/load_from_parquet\",\n",
    "    arguments={\n",
    "        \"dataset_uri\": \"../../foobar/sample.parquet\",\n",
    "    },\n",
    "    produces={\"x\": pa.int32(), \"y\": pa.int32()},\n",
    ")\n",
    "\n",
    "\n",
    "class LoadFromParquett(DaskLoadComponent):\n",
    "    def __init__(self, *_, **__):\n",
    "        pass\n",
    "    def load(self) -> dd.DataFrame:\n",
    "        dask_df = dd.read_parquet(\"./foobar/sample.parquet\", columns=[\"x\", \"y\"])\n",
    "        return dask_df\n",
    "\n",
    "\n",
    "dataset1 = dataset.execute(\n",
    "    component=LoadFromParquett,\n",
    "    produces={\"x\": pa.int32(), \"y\": pa.int32()},\n",
    ")\n",
    "\n",
    "\n",
    "class Addition(PandasTransformComponent):\n",
    "    def __init__(self, *_, **__):\n",
    "        pass\n",
    "\n",
    "    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        dataframe['z'] = dataframe['x'] + dataframe['y']\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "dataset2 = dataset1.execute(\n",
    "    component=Addition,\n",
    "    produces={\"z\": pa.int32()},\n",
    "    consumes={\"x\": pa.int32(), \"y\": pa.int32()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
